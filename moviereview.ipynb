{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPnZAKFNZN1MhW3hY6JY58u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/overhaulk/image-classification-ml-model/blob/main/moviereview.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pujEkaHrTHS1",
        "outputId": "5c6f8267-f795-44d7-df73-ee7a0cc56565"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"IMDB Dataset.csv\")\n",
        "print(df.head())\n",
        "print(df[\"sentiment\"].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgbtoWLITib6",
        "outputId": "8ecb5574-9cd1-44b3-f5c0-24738adc5e31"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              review sentiment\n",
            "0  One of the other reviewers has mentioned that ...  positive\n",
            "1  A wonderful little production. <br /><br />The...  positive\n",
            "2  I thought this was a wonderful way to spend ti...  positive\n",
            "3  Basically there's a family where a little boy ...  negative\n",
            "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
            "sentiment\n",
            "positive    25000\n",
            "negative    25000\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 200\n",
        "VOCAB_SIZE = 20000\n",
        "\n",
        "stop_word = set(stopwords.words(\"english\"))\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"<.*?>\", \" \", text)\n",
        "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [w for w in tokens if w not in stop_words]\n",
        "    return tokens\n",
        "\n",
        "df[\"tokens\"] = df[\"review\"].apply(clean_text)"
      ],
      "metadata": {
        "id": "mOnkALQ2T8Vl"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "all_tokens = [token for tokens in df[\"tokens\"] for token in tokens]\n",
        "freqs = Counter(all_tokens)\n",
        "vocab = {word: i+2 for i, (word, _) in enumerate(freqs.most_common(VOCAB_SIZE))}\n",
        "vocab[\"<PAD>\"] = 0\n",
        "vocab[\"<UNK>\"] = 1\n",
        "\n",
        "def encode(tokens):\n",
        "    return [vocab.get(w, 1) for w in tokens[:MAX_LEN]] + [0]*(MAX_LEN - len(tokens))\n",
        "\n",
        "df[\"encoded\"] = df[\"tokens\"].apply(encode)\n",
        "df[\"label\"] = df[\"sentiment\"].apply(lambda x: 1 if x == \"positive\" else 0)"
      ],
      "metadata": {
        "id": "AVr7hAHgUlqI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class IMDBDataset(Dataset):\n",
        "  def __init__(self, reviews, labels):\n",
        "        self.reviews = reviews\n",
        "        self.labels = labels\n",
        "\n",
        "  def __len__(self):\n",
        "        return len(self.reviews)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "        return torch.tensor(self.reviews[idx], dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.float)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(df[\"encoded\"], df[\"label\"], test_size=0.2, random_state=42)\n",
        "\n",
        "train_dataset = IMDBDataset(X_train.tolist(), y_train.tolist())\n",
        "val_dataset = IMDBDataset(X_val.tolist(), y_val.tolist())\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64)"
      ],
      "metadata": {
        "id": "qcKTHDvjWgVB"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNTextClassifier(nn.Module):\n",
        "  def __init__(self, vocab_size, embed_dim=128, num_classes=1):\n",
        "    super(CNNTextClassifier, self).__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "    self.conv1 = nn.Conv1d(embed_dim, 128, kernel_size=3, padding=1)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.pool = nn.AdaptiveMaxPool1d(1)\n",
        "    self.fc = nn.Linear(128, num_classes)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.pool(x).squeeze(2)\n",
        "        x = self.fc(x)\n",
        "        return self.sigmoid(x).squeeze()"
      ],
      "metadata": {
        "id": "LvpRgPFIWgQN"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = CNNTextClassifier(len(vocab)).to(device)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "EPOCHS = 30\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss, correct = 0, 0\n",
        "    for X, y in train_loader:\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X)\n",
        "        loss = criterion(outputs, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        correct += ((outputs >= 0.5).float() == y).sum().item()\n",
        "\n",
        "    acc = correct / len(train_dataset)\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}, Train Acc: {acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1jRhVCxWgN2",
        "outputId": "039df954-2a12-4be3-9d44-6be3f3cc67f8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 268.6170, Train Acc: 0.7981\n",
            "Epoch 2, Loss: 155.8785, Train Acc: 0.9021\n",
            "Epoch 3, Loss: 88.0719, Train Acc: 0.9550\n",
            "Epoch 4, Loss: 39.3363, Train Acc: 0.9886\n",
            "Epoch 5, Loss: 14.2444, Train Acc: 0.9986\n",
            "Epoch 6, Loss: 5.4826, Train Acc: 1.0000\n",
            "Epoch 7, Loss: 2.5505, Train Acc: 1.0000\n",
            "Epoch 8, Loss: 1.4475, Train Acc: 1.0000\n",
            "Epoch 9, Loss: 0.8847, Train Acc: 1.0000\n",
            "Epoch 10, Loss: 0.5647, Train Acc: 1.0000\n",
            "Epoch 11, Loss: 0.3707, Train Acc: 1.0000\n",
            "Epoch 12, Loss: 0.2466, Train Acc: 1.0000\n",
            "Epoch 13, Loss: 0.1662, Train Acc: 1.0000\n",
            "Epoch 14, Loss: 0.1126, Train Acc: 1.0000\n",
            "Epoch 15, Loss: 0.0769, Train Acc: 1.0000\n",
            "Epoch 16, Loss: 0.0529, Train Acc: 1.0000\n",
            "Epoch 17, Loss: 0.0365, Train Acc: 1.0000\n",
            "Epoch 18, Loss: 0.0253, Train Acc: 1.0000\n",
            "Epoch 19, Loss: 0.0176, Train Acc: 1.0000\n",
            "Epoch 20, Loss: 0.0123, Train Acc: 1.0000\n",
            "Epoch 21, Loss: 0.0086, Train Acc: 1.0000\n",
            "Epoch 22, Loss: 0.0060, Train Acc: 1.0000\n",
            "Epoch 23, Loss: 0.0042, Train Acc: 1.0000\n",
            "Epoch 24, Loss: 0.0030, Train Acc: 1.0000\n",
            "Epoch 25, Loss: 0.0021, Train Acc: 1.0000\n",
            "Epoch 26, Loss: 0.0015, Train Acc: 1.0000\n",
            "Epoch 27, Loss: 0.0011, Train Acc: 1.0000\n",
            "Epoch 28, Loss: 0.0008, Train Acc: 1.0000\n",
            "Epoch 29, Loss: 0.0005, Train Acc: 1.0000\n",
            "Epoch 30, Loss: 0.0004, Train Acc: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "    for X, y in val_loader:\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        outputs = model(X)\n",
        "        correct += ((outputs >= 0.5).float() == y).sum().item()\n",
        "\n",
        "print(\"Validation Accuracy:\", correct / len(val_dataset))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBx2tBTiWgLb",
        "outputId": "cf8163ff-5b74-48f9-a430-5d1db80e9b1e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.8831\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# after training finishes\n",
        "torch.save(model.state_dict(), \"sentiment_cnn.pth\")\n"
      ],
      "metadata": {
        "id": "eusvYOy8WgGm"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- Function to predict user review ----\n",
        "def predict_review(model, review, vocab, max_len=200):\n",
        "    model.eval()\n",
        "    # Clean the review\n",
        "    review = review.lower()\n",
        "    review = re.sub(r\"<.*?>\", \" \", review)\n",
        "    review = review.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "    tokens = word_tokenize(review)\n",
        "    tokens = [w for w in tokens if w not in stop_word]\n",
        "\n",
        "    # Encode\n",
        "    encoded = [vocab.get(w, 1) for w in tokens[:max_len]] + [0] * (max_len - len(tokens))\n",
        "    encoded = torch.tensor(encoded, dtype=torch.long).unsqueeze(0).to(device)  # batch of 1\n",
        "\n",
        "    # Prediction\n",
        "    with torch.no_grad():\n",
        "        output = model(encoded).item()\n",
        "\n",
        "    sentiment = \"Positive ðŸ˜Š\" if output >= 0.5 else \"Negative ðŸ˜ž\"\n",
        "    return sentiment, output\n",
        "\n",
        "# ---- Take input from user ----\n",
        "user_review = input(\"Enter a movie review: \")\n",
        "sentiment, score = predict_review(model, user_review, vocab)\n",
        "print(f\"Review Sentiment: {sentiment} (score={score:.4f})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOvu0v0bWgD2",
        "outputId": "1b056b9e-c1cd-4938-bfc9-4105a9d2a25b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a movie review: it was shit good movie\n",
            "Review Sentiment: Positive ðŸ˜Š (score=1.0000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MakbbtWiWgBJ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dZhiqtWCWf8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F4-CK0M0Wf6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CFQut0adWfwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JVYnPUbUWfkK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}